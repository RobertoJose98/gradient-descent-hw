{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlUvdJBiUGEb"
      },
      "source": [
        "# Homework 0\n",
        "\n",
        "**Name:** -- Roberto José González --\n",
        "\n",
        "**e-mail:** -- roberto.jose0745@alumnos.udg.mx --"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GsD8XUcUGEe"
      },
      "source": [
        "# Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "50jI8nsnUGEf"
      },
      "outputs": [],
      "source": [
        "# Load modules\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSH5PpPpUGEg"
      },
      "source": [
        "# Theory on the Gradient Descent algorithm\n",
        "\n",
        "Gradient Descent is an optimization algorithm used to find the local minimum of a function by iteratively moving in the direction of the negative gradient. The update rule is:\n",
        "\n",
        "$$\n",
        "x_{t+1} = x_t - \\alpha \\frac{\\partial f}{\\partial x}, \\quad y_{t+1} = y_t - \\alpha \\frac{\\partial f}{\\partial y}\n",
        "$$\n",
        "\n",
        "where:\n",
        "- \\$ (x_t, y_t) \\$ is the current position,\n",
        "- \\$\\alpha \\$ is the learning rate,\n",
        "- \\$ \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\$ are the partial derivatives.\n",
        "\n",
        "The algorithm stops when the change in function value is very small or after a set number of iterations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "e_LPKHbuUGEg"
      },
      "outputs": [],
      "source": [
        "# Function to be optimized\n",
        "def himmelblau(x, y):\n",
        "    return (x**2 + y - 11)**2 + (x + y**2 - 7)**2\n",
        "\n",
        "# Gradient of the function\n",
        "def gradient(x, y):\n",
        "    df_dx = 2 * (x**2 + y - 11) * 2 * x + 2 * (x + y**2 - 7)\n",
        "    df_dy = 2 * (x**2 + y - 11) + 2 * (x + y**2 - 7) * 2 * y\n",
        "    return np.array([df_dx, df_dy])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nP3sKeoDUGEg"
      },
      "outputs": [],
      "source": [
        "# Run gradient descent algorithm\n",
        "def gradient_descent(start_x, start_y, learning_rate=0.01, max_iters=1000, tol=1e-6):\n",
        "    x, y = start_x, start_y\n",
        "    history = [(x, y)]\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        grad = gradient(x, y)\n",
        "        x_new = x - learning_rate * grad[0]\n",
        "        y_new = y - learning_rate * grad[1]\n",
        "\n",
        "        history.append((x_new, y_new))\n",
        "\n",
        "        # Convergence check\n",
        "        if np.linalg.norm([x_new - x, y_new - y]) < tol:\n",
        "            break\n",
        "\n",
        "        x, y = x_new, y_new\n",
        "\n",
        "    return np.array(history)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "iEG3NUMHUGEh"
      },
      "outputs": [],
      "source": [
        "# Plot the results\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}