{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlUvdJBiUGEb"
      },
      "source": [
        "# Homework 0\n",
        "\n",
        "**Name:** -- Roberto Jos√© Gonz√°lez --\n",
        "\n",
        "**e-mail:** -- roberto.jose0745@alumnos.udg.mx --"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GsD8XUcUGEe"
      },
      "source": [
        "# Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "50jI8nsnUGEf"
      },
      "outputs": [],
      "source": [
        "# Load modules\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSH5PpPpUGEg"
      },
      "source": [
        "# Theory on the Gradient Descent algorithm\n",
        "\n",
        "Gradient Descent is an optimization algorithm used to find the local minimum of a function by iteratively moving in the direction of the negative gradient. The update rule is:\n",
        "\n",
        "$$\n",
        "x_{t+1} = x_t - \\alpha \\frac{\\partial f}{\\partial x}, \\quad y_{t+1} = y_t - \\alpha \\frac{\\partial f}{\\partial y}\n",
        "$$\n",
        "\n",
        "where:\n",
        "- \\$ (x_t, y_t) \\$ is the current position,\n",
        "- \\$\\alpha \\$ is the learning rate,\n",
        "- \\$ \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\$ are the partial derivatives.\n",
        "\n",
        "The algorithm stops when the change in function value is very small or after a set number of iterations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to be optimized  \n",
        "\n",
        "The function to be optimized is the **Himmelblau function**, defined as:  \n",
        "\n",
        "$$\n",
        "f(x, y) = (x^2 + y - 11)^2 + (x + y^2 - 7)^2\n",
        "$$\n",
        "\n",
        "The gradient of the function is given by:  \n",
        "\n",
        "$\n",
        "\\frac{\\partial f}{\\partial x} = 2 (x^2 + y - 11) \\cdot 2x + 2 (x + y^2 - 7)\n",
        "$\n",
        "\n",
        "$\n",
        "\\frac{\\partial f}{\\partial y} = 2 (x^2 + y - 11) + 2 (x + y^2 - 7) \\cdot 2y\n",
        "$\n",
        "\n",
        "This gradient will be used in the **Gradient Descent Algorithm** to iteratively find a local minimum.\n"
      ],
      "metadata": {
        "id": "dvjmVhOT0J8M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "e_LPKHbuUGEg"
      },
      "outputs": [],
      "source": [
        "# Function to be optimized\n",
        "def himmelblau(x, y):\n",
        "    return (x**2 + y - 11)**2 + (x + y**2 - 7)**2\n",
        "\n",
        "# Gradient of the function\n",
        "def gradient(x, y):\n",
        "    df_dx = 2 * (x**2 + y - 11) * 2 * x + 2 * (x + y**2 - 7)\n",
        "    df_dy = 2 * (x**2 + y - 11) + 2 * (x + y**2 - 7) * 2 * y\n",
        "    return np.array([df_dx, df_dy])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running the Gradient Descent Algorithm  \n",
        "\n",
        "The **Gradient Descent Algorithm** is used to iteratively update $ x $ and $ y $ in the direction of the negative gradient to minimize the Himmelblau function. The update rule is given by:  \n",
        "\n",
        "$$\n",
        "x_{t+1} = x_t - \\alpha \\frac{\\partial f}{\\partial x}\n",
        "$$\n",
        "\n",
        "$$\n",
        "y_{t+1} = y_t - \\alpha \\frac{\\partial f}{\\partial y}\n",
        "$$\n",
        "\n",
        "where:  \n",
        "- $ (x_t, y_t) $ is the current position,  \n",
        "- $ \\alpha $ is the learning rate,  \n",
        "- $ \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} $ are the partial derivatives of the function.  \n",
        "\n",
        "# Stopping Criteria  \n",
        "The algorithm stops when either:  \n",
        "1. The maximum number of iterations (**max_iters**) is reached.  \n",
        "2. The change in position is smaller than a predefined tolerance (**tol**), computed as:  \n",
        "\n",
        "$$\n",
        "\\sqrt{(x_{t+1} - x_t)^2 + (y_{t+1} - y_t)^2} < \\text{tol}\n",
        "$$\n",
        "\n",
        "The function returns the history of all positions visited during optimization.\n"
      ],
      "metadata": {
        "id": "-kXgGn7a0u8S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nP3sKeoDUGEg"
      },
      "outputs": [],
      "source": [
        "# Run gradient descent algorithm\n",
        "def gradient_descent(start_x, start_y, learning_rate=0.01, max_iters=1000, tol=1e-6):\n",
        "\n",
        "    # Initialize (x, y) with the given starting point\n",
        "    x, y = start_x, start_y\n",
        "\n",
        "    # Store the history of positions for visualization\n",
        "    history = [(x, y)]\n",
        "\n",
        "    # Iterate up to max_iters times\n",
        "    for i in range(max_iters):\n",
        "        # Partial derivatives\n",
        "        grad = gradient(x, y)\n",
        "\n",
        "        # Update x and y based on the gradient descent rule\n",
        "        x_new = x - learning_rate * grad[0]\n",
        "        y_new = y - learning_rate * grad[1]\n",
        "\n",
        "        # Save the new position\n",
        "        history.append((x_new, y_new))\n",
        "\n",
        "        # üõë Check for convergence:\n",
        "        # Stop if the change in position is smaller than the tolerance\n",
        "        if np.linalg.norm([x_new - x, y_new - y]) < tol:\n",
        "            break  # Exit the loop\n",
        "\n",
        "        # Update (x, y) for the next iteration\n",
        "        x, y = x_new, y_new\n",
        "\n",
        "    # Convert history list to a NumPy array for easier processing\n",
        "    return np.array(history)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "iEG3NUMHUGEh"
      },
      "outputs": [],
      "source": [
        "# Plot the results\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}